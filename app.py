import json
import datetime
from typing import Any
from pathlib import Path

import pandas as pd
import streamlit as st
from elasticsearch import Elasticsearch

# Geminié–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from gemini_helper import get_gemini_model, generate_summary
from prompt import get_summary_prompt, get_custom_prompt

# ====== Config (Streamlit Secrets) ======
def get_secret(key: str, default: str = "") -> str:
    """Streamlit Secretsã‹ã‚‰å€¤ã‚’å–å¾—ã€‚å­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™"""
    try:
        return st.secrets.get(key, default)
    except Exception:
        return default

def _check_password() -> bool:
    """APP_PASSWORD ã‚’ä½¿ã£ãŸè¶…ã‚·ãƒ³ãƒ—ãƒ«ãªã‚²ãƒ¼ãƒˆã€‚
    - APP_PASSWORD ãŒç„¡ã„/ç©º â†’ èªè¨¼ã‚ªãƒ•ï¼ˆãã®ã¾ã¾å…¥ã‚Œã‚‹ï¼‰
    - åˆã£ã¦ã„ã‚Œã° session_state ã«è¨˜éŒ²ã—ã¦ä»¥å¾Œã‚¹ãƒ«ãƒ¼
    """
    required_pw = get_secret("APP_PASSWORD")
    if not required_pw:  # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰æœªè¨­å®šãªã‚‰èªè¨¼OFFï¼ˆé–‹ç™ºç”¨ï¼‰
        return True

    # ã™ã§ã«ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ï¼Ÿ
    if st.session_state.get("_authed", False):
        return True

    # å…¥åŠ›UIï¼ˆãƒšãƒ¼ã‚¸ã®å…ˆé ­ã«è¡¨ç¤ºï¼‰
    with st.container():
        st.markdown("### ğŸ” ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        pw = st.text_input("Password", type="password", placeholder="Enter password", help="é‹ç”¨æ‹…å½“ã‹ã‚‰å…±æœ‰ã•ã‚ŒãŸãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›")
        col_a, col_b = st.columns([1,5])
        with col_a:
            submit = st.button("ãƒ­ã‚°ã‚¤ãƒ³", use_container_width=True)
        if submit:
            if pw == required_pw:
                st.session_state["_authed"] = True
                st.rerun()
            else:
                st.error("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé•ã„ã¾ã™ã€‚")

    return False

# ã“ã“ã§ã‚²ãƒ¼ãƒˆã€‚é€šã‚Œãªã‘ã‚Œã°ä»¥é™ã‚’å®Ÿè¡Œã—ãªã„
if not _check_password():
    st.stop()

# ====== Page & CSS ======
st.set_page_config(page_title="G-Finder ãƒ‡ãƒ¼ã‚¿åéŒ²çŠ¶æ³", layout="wide")
st.markdown("""
<style>
  [data-testid="stSidebar"] {width: 360px;}
  [data-testid="stSidebar"] section {width: 360px;}
</style>
""", unsafe_allow_html=True)

# ====== Elasticsearchæ¥ç¶šæƒ…å ±ã‚’å–å¾— ======
ES_HOST = get_secret("ES_HOST")
ES_USERNAME = get_secret("ES_USERNAME")
ES_PASSWORD = get_secret("ES_PASSWORD")
ES_INDEX_yosankessan = get_secret("ES_INDEX_yosankessan")
ES_INDEX_keikakuhoshin = get_secret("ES_INDEX_keikakuhoshin")
ES_INDEX_iinkaigijiroku = get_secret("ES_INDEX_iinkaigijiroku")
ES_INDEX_kouhou = get_secret("ES_INDEX_kouhou")
INDEXES = [i for i in [ES_INDEX_yosankessan, ES_INDEX_keikakuhoshin, ES_INDEX_iinkaigijiroku, ES_INDEX_kouhou] if i]

# Gemini APIã‚­ãƒ¼ã‚’å–å¾—
GEMINI_API_KEY = get_secret("GEMINI_API_KEY")

@st.cache_resource(show_spinner=False)
def es_client() -> Elasticsearch:
    if not ES_HOST or not ES_USERNAME or not ES_PASSWORD:
        st.error("ES æ¥ç¶šæƒ…å ±ãŒä¸è¶³ï¼ˆES_HOST / ES_USERNAME / ES_PASSWORDï¼‰")
        st.stop()
    return Elasticsearch(ES_HOST, basic_auth=(ES_USERNAME, ES_PASSWORD), verify_certs=False, request_timeout=90)
es = es_client()

# ====== Masters ======
# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—ï¼ˆã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¾ãŸã¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
def get_data_path(filename: str) -> Path:
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—"""
    # ã¾ãšã€ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚§ãƒƒã‚¯
    current_path = Path(filename)
    if current_path.exists():
        return current_path
    
    # æ¬¡ã«ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚§ãƒƒã‚¯
    script_dir = Path(__file__).parent
    script_path = script_dir / filename
    if script_path.exists():
        return script_path
    
    # ã©ã¡ã‚‰ã‚‚å­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
    raise FileNotFoundError(
        f"'{filename}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
        f"ç¢ºèªã—ãŸãƒ‘ã‚¹:\n"
        f"  - {current_path.absolute()}\n"
        f"  - {script_path.absolute()}"
    )

@st.cache_data(show_spinner=False)
def load_jichitai() -> pd.DataFrame:
    try:
        filepath = get_data_path("jichitai.xlsx")
        df = pd.read_excel(filepath, dtype={"code": str, "affiliation_code": str})
    except FileNotFoundError as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()
    except Exception as e:
        st.error(f"jichitai.xlsx ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()
    
    need = ["code", "affiliation_code", "pref_name", "city_name", "city_type"]
    miss = [c for c in need if c not in df.columns]
    if miss:
        st.error(f"jichitai.xlsx ã«å¿…é ˆåˆ—ãŒä¸è¶³: {miss}")
        st.stop()
    df["code"] = df["code"].str.zfill(6)
    df["affiliation_code"] = df["affiliation_code"].str.zfill(2)  # 2æ¡ã§çµ±ä¸€
    return df[need]

@st.cache_data(show_spinner=False)
def load_category() -> pd.DataFrame:
    try:
        filepath = get_data_path("category.xlsx")
        df = pd.read_excel(filepath)
    except FileNotFoundError as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()
    except Exception as e:
        st.error(f"category.xlsx ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()
    
    need = ["category", "category_name", "short_name", "order"]
    miss = [c for c in need if c not in df.columns]
    if miss:
        st.error(f"category.xlsx ã«å¿…é ˆåˆ—ãŒä¸è¶³: {miss}")
        st.stop()
    if "group" not in df.columns:
        df["group"] = ""
    df = df.astype({"category": int, "order": int})
    return df

jichitai = load_jichitai()
catmap = load_category()
pref_master = (
    jichitai[["affiliation_code", "pref_name"]]
    .drop_duplicates()
    .assign(aff_num=lambda d: pd.to_numeric(d["affiliation_code"], errors="coerce"))
)

# ====== ES fields ======
FIELD_CODE = "code"
FIELD_AFFILIATION = "affiliation_code"
FIELD_CATEGORY = "category"
FIELD_FILE_ID = "file_id"
FIELD_COLLECTED_AT = "collected_at"  # datetime

# ====== Sidebar ======

# ========== ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»å¹´åº¦æ¤œç´¢ ===========
st.sidebar.subheader("ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»å¹´åº¦çµã‚Šè¾¼ã¿")

year_options = list(range(2010, 2031))
selected_years = st.sidebar.multiselect(
    "å¹´åº¦ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
    options=year_options,
    default=[],
    help="fiscal_year_start/fiscal_year_endã§çµã‚Šè¾¼ã¿"
)

and_input = st.sidebar.text_input(
    "ANDæ¡ä»¶ï¼ˆã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šï¼‰",
    placeholder="ä¾‹: ç’°å¢ƒ è¨ˆç”»",
    help="å…¨ã¦ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€æ–‡æ›¸ã‚’æ¤œç´¢"
)
or_input = st.sidebar.text_input(
    "ORæ¡ä»¶ï¼ˆã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šï¼‰",
    placeholder="ä¾‹: æ¸©æš–åŒ– æ°—å€™å¤‰å‹•",
    help="ã„ãšã‚Œã‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€æ–‡æ›¸ã‚’æ¤œç´¢"
)
not_input = st.sidebar.text_input(
    "NOTæ¡ä»¶ï¼ˆã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šï¼‰",
    placeholder="ä¾‹: å»ƒæ­¢ ä¸­æ­¢",
    help="ã“ã‚Œã‚‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã¾ãªã„æ–‡æ›¸ã‚’æ¤œç´¢"
)

search_title = st.sidebar.checkbox(
    "è³‡æ–™åã‚‚æ¤œç´¢å¯¾è±¡ã«å«ã‚ã‚‹",
    value=False,
    help="ãƒã‚§ãƒƒã‚¯ã‚’å…¥ã‚Œã‚‹ã¨titleãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚æ¤œç´¢å¯¾è±¡ã«ãªã‚Šã¾ã™"
)

st.sidebar.markdown("---")

# ========== è‡ªæ²»ä½“çµã‚Šè¾¼ã¿ ==========
st.sidebar.subheader("ğŸ” è‡ªæ²»ä½“ãƒ»ã‚«ãƒ†ã‚´ãƒªçµã‚Šè¾¼ã¿")
pref_opts = (
    jichitai[["affiliation_code", "pref_name"]]
    .drop_duplicates().assign(aff_num=lambda d: pd.to_numeric(d["affiliation_code"], errors="coerce"))
    .sort_values(["aff_num"])
)
sel_pref_names = st.sidebar.multiselect("éƒ½é“åºœçœŒ", options=pref_opts["pref_name"].tolist())
sel_aff_codes = pref_opts[pref_opts["pref_name"].isin(sel_pref_names)]["affiliation_code"].tolist()

ctype_opts = sorted(jichitai["city_type"].dropna().unique().tolist())
sel_city_types = st.sidebar.multiselect("è‡ªæ²»ä½“åŒºåˆ†", options=ctype_opts)

if sel_aff_codes:
    city_pool = jichitai[jichitai["affiliation_code"].isin(sel_aff_codes)]
else:
    city_pool = jichitai.copy()
if sel_city_types:
    city_pool = city_pool[city_pool["city_type"].isin(sel_city_types)]
city_pool = city_pool.sort_values(["affiliation_code", "code"])
sel_city_names = st.sidebar.multiselect("å¸‚åŒºç”ºæ‘", options=city_pool["city_name"].tolist())
sel_codes = city_pool[city_pool["city_name"].isin(sel_city_names)]["code"].tolist()

cat_opts = catmap.sort_values("order")
short_unique = cat_opts.drop_duplicates(subset=["short_name"], keep="first")
sel_cat_short = st.sidebar.multiselect("è³‡æ–™ã‚«ãƒ†ã‚´ãƒª", options=short_unique["short_name"].tolist(), default=short_unique["short_name"].tolist())
sel_categories = cat_opts[cat_opts["short_name"].isin(sel_cat_short)]["category"].astype(int).tolist()


# ========== è¡¨ç¤ºè¨­å®š ==========
st.sidebar.markdown("---")
st.sidebar.header("è¡¨ç¤ºè¨­å®š")
display_unit = st.sidebar.radio(
    "è¡¨ç¤ºå˜ä½", 
    ["éƒ½é“åºœçœŒ", "å¸‚åŒºç”ºæ‘"],
    index=0)
count_mode = st.sidebar.radio(
    "é›†è¨ˆå˜ä½", 
    ["ãƒ•ã‚¡ã‚¤ãƒ«æ•°", "ãƒšãƒ¼ã‚¸æ•°"], 
    index=0,
    help="ãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼šPDFãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã§é›†è¨ˆ\nãƒšãƒ¼ã‚¸æ•°ï¼šPDFã®ãƒšãƒ¼ã‚¸å˜ä½ã§é›†è¨ˆ")
result_limit = st.sidebar.radio(
    "æ¤œç´¢çµæœã®è¡¨ç¤ºä»¶æ•°",
    options=[100, 1000, 10000],
    index=0,
    help="æ¤œç´¢çµæœã‚¿ãƒ–ã§ã®è¡¨ç¤ºä»¶æ•°ã‚’å¤‰æ›´ã§ãã¾ã™\nï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100ä»¶ å¤šããªã‚‹ã¨æŒ™å‹•ãŒé‡ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰"
)

# ====== Query Builder ======
def build_search_query(and_words, or_words, not_words, years, codes, categories, include_title=False):
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»å¹´åº¦ãƒ»è‡ªæ²»ä½“ãƒ»ã‚«ãƒ†ã‚´ãƒªã‚’çµ„ã¿åˆã‚ã›ãŸã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰"""
    must_clauses = []
    should_clauses = []
    must_not_clauses = []
    filter_clauses = []
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
    for w in and_words:
        if include_title:
            must_clauses.append({
                "bool": {
                    "should": [
                        {"match_phrase": {"content_text": w}},
                        {"match_phrase": {"title": w}}
                    ],
                    "minimum_should_match": 1
                }
            })
        else:
            must_clauses.append({"match_phrase": {"content_text": w}})
    
    for w in or_words:
        if include_title:
            should_clauses.append({
                "bool": {
                    "should": [
                        {"match_phrase": {"content_text": w}},
                        {"match_phrase": {"title": w}}
                    ],
                    "minimum_should_match": 1
                }
            })
        else:
            should_clauses.append({"match_phrase": {"content_text": w}})
    
    for w in not_words:
        if include_title:
            must_not_clauses.append({
                "bool": {
                    "should": [
                        {"match_phrase": {"content_text": w}},
                        {"match_phrase": {"title": w}}
                    ],
                    "minimum_should_match": 1
                }
            })
        else:
            must_not_clauses.append({"match_phrase": {"content_text": w}})
    
    # å¹´åº¦æ¤œç´¢
    if years:
        year_should = []
        for y in years:
            # fiscal_year_start <= y <= fiscal_year_end
            cond_between = {
                "bool": {
                    "must": [
                        {"range": {"fiscal_year_start": {"lte": y}}},
                        {"range": {"fiscal_year_end": {"gte": y}}}
                    ]
                }
            }
            # fiscal_year_start == y ã‹ã¤ fiscal_year_end ãŒå­˜åœ¨ã—ãªã„
            cond_start_eq_when_no_end = {
                "bool": {
                    "must": [
                        {"term": {"fiscal_year_start": y}}
                    ],
                    "must_not": [
                        {"exists": {"field": "fiscal_year_end"}}
                    ]
                }
            }
            year_should.append(cond_between)
            year_should.append(cond_start_eq_when_no_end)
        
        filter_clauses.append({
            "bool": {
                "should": year_should,
                "minimum_should_match": 1
            }
        })
    
    # è‡ªæ²»ä½“ã‚³ãƒ¼ãƒ‰
    if codes:
        filter_clauses.append({"terms": {FIELD_CODE: codes}})
    
    # ã‚«ãƒ†ã‚´ãƒª
    if categories:
        filter_clauses.append({"terms": {FIELD_CATEGORY: categories}})
    
    # ã‚¯ã‚¨ãƒªçµ„ã¿ç«‹ã¦
    query = {"bool": {}}
    if must_clauses:
        query["bool"]["must"] = must_clauses
    if should_clauses:
        query["bool"]["should"] = should_clauses
        query["bool"]["minimum_should_match"] = 1
    if must_not_clauses:
        query["bool"]["must_not"] = must_not_clauses
    if filter_clauses:
        query["bool"]["filter"] = filter_clauses
    
    # ä½•ã‚‚æ¡ä»¶ãŒãªã„å ´åˆ
    if not query["bool"]:
        return {"match_all": {}}
    
    return query

# ====== Query (common) ======
code_pool = jichitai.copy()
if sel_aff_codes:
    code_pool = code_pool[code_pool["affiliation_code"].isin(sel_aff_codes)]
if sel_city_types:
    code_pool = code_pool[code_pool["city_type"].isin(sel_city_types)]
if sel_city_names:
    code_pool = code_pool[code_pool["city_name"].isin(sel_city_names)]
codes_for_query = code_pool["code"].tolist()

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡¦ç†
and_words = [w.strip() for w in and_input.replace("ã€€", " ").split() if w.strip()]
or_words = [w.strip() for w in or_input.replace("ã€€", " ").split() if w.strip()]
not_words = [w.strip() for w in not_input.replace("ã€€", " ").split() if w.strip()]

# ã‚¯ã‚¨ãƒªæ§‹ç¯‰
query = build_search_query(
    and_words=and_words,
    or_words=or_words,
    not_words=not_words,
    years=selected_years,
    codes=codes_for_query,
    categories=sel_categories,
    include_title=search_title
)

# ====== KPIï¼ˆå…¨ä½“ï¼‰ ======
kpi_body = {
    "size": 0,
    "track_total_hits": True,
    "query": query,
    "aggs": {
        "uniq_files": {"cardinality": {"field": FIELD_FILE_ID, "precision_threshold": 40000}},
        "max_collected": {"max": {"field": FIELD_COLLECTED_AT}},
    },
}
kpi_res = es.search(index=INDEXES, body=kpi_body)
kpi_total_pages = kpi_res.get("hits", {}).get("total", {}).get("value", 0)
kpi_total_files = kpi_res.get("aggregations", {}).get("uniq_files", {}).get("value", 0)
max_collected_value = kpi_res.get("aggregations", {}).get("max_collected", {}).get("value")

def fmt_month_from_epoch(v):
    if v is None: return "â€•"
    try:
        dt = datetime.datetime.utcfromtimestamp(v/1000.0) + datetime.timedelta(hours=9)
        return f"{dt.year}å¹´{dt.month}æœˆ"
    except Exception:
        return "â€•"

latest_collected_label = fmt_month_from_epoch(max_collected_value)

# ====== Title + KPI ======
st.markdown("""
# G-Finder Liteâš¡ 
ãƒ»å„åˆ—ã®ãƒ˜ãƒƒãƒ€ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ä¸¦ã³æ›¿ãˆã§ãã¾ã™ã€‚  
ãƒ»æœ€æ–°åé›†æœˆã¯åé›†è€…ãŒæœ€å¾Œã«åé›†ã—ãŸæ—¥ä»˜ã‹ã‚‰ç®—å‡ºã—ã¦ã„ã‚‹ãŸã‚ã€å¿…ãšã—ã‚‚å½“æœˆã®è³‡æ–™ãŒåéŒ²ã•ã‚Œã¦ã„ã‚‹ã¨ã„ã†ã“ã¨ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚  
ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯å®Œå…¨ä¸€è‡´æ¤œç´¢ã§ã™ï¼ˆ""ã¯ä¸è¦ã§ã™ï¼‰
""", unsafe_allow_html=True)

# æ¤œç´¢æ¡ä»¶ã®è¡¨ç¤º
search_info_parts = []
if and_words:
    search_info_parts.append(f"**AND**: {', '.join(and_words)}")
if or_words:
    search_info_parts.append(f"**OR**: {', '.join(or_words)}")
if not_words:
    search_info_parts.append(f"**NOT**: {', '.join(not_words)}")
if selected_years:
    search_info_parts.append(f"**å¹´åº¦**: {', '.join(map(str, sorted(selected_years)))}")
if search_title:
    search_info_parts.append("**æ¤œç´¢å¯¾è±¡**: æœ¬æ–‡ + è³‡æ–™å")

if search_info_parts:
    st.info("ğŸ” **æ¤œç´¢æ¡ä»¶**: " + " | ".join(search_info_parts))

k1, k2, _sp = st.columns([2, 2, 6])
with k1: st.metric("ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°", f"{kpi_total_files:,}")
with k2: st.metric("ç·ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒšãƒ¼ã‚¸ï¼‰æ•°", f"{kpi_total_pages:,}")

# ====== Helpers ======
def _qkey(obj: Any) -> str:
    return json.dumps(obj, sort_keys=True, ensure_ascii=False)

@st.cache_data(show_spinner=False, ttl=300)
def fetch_counts(query_key: str, group_field: str, include_file: bool) -> pd.DataFrame:
    after, recs = None, []
    while True:
        body = {
            "size": 0,
            "query": json.loads(query_key) if query_key else {"match_all": {}},
            "aggs": {
                "by_pair": {
                    "composite": {
                        "size": 500,
                        "sources": [
                            {"g": {"terms": {"field": group_field}}},
                            {"category": {"terms": {"field": FIELD_CATEGORY}}},
                        ],
                        **({"after": after} if after else {}),
                    },
                    "aggs": ({ "file_count": {"cardinality": {"field": FIELD_FILE_ID}} } if include_file else {}),
                }
            },
        }
        res = es.search(index=INDEXES, body=body)
        for b in res["aggregations"]["by_pair"]["buckets"]:
            recs.append({
                "g": str(b["key"]["g"]),
                "category": int(b["key"]["category"]) if b["key"].get("category") is not None else None,
                "page_docs": b["doc_count"],
                "file_docs": b.get("file_count", {}).get("value", 0),
            })
        after = res["aggregations"]["by_pair"].get("after_key")
        if not after: break
    return pd.DataFrame.from_records(recs)

@st.cache_data(show_spinner=False, ttl=300)
def fetch_latest_month(query_key: str, group_field: str) -> pd.DataFrame:
    """gÃ—categoryã”ã¨ã® collected_at æœ€å¤§ï¼ˆepoch millisï¼‰"""
    after, recs = None, []
    while True:
        body = {
            "size": 0,
            "query": json.loads(query_key) if query_key else {"match_all": {}},
            "aggs": {
                "by_pair": {
                    "composite": {
                        "size": 500,
                        "sources": [
                            {"g": {"terms": {"field": group_field}}},
                            {"category": {"terms": {"field": FIELD_CATEGORY}}},
                        ],
                        **({"after": after} if after else {}),
                    },
                    "aggs": { "max_collected": { "max": { "field": FIELD_COLLECTED_AT } } }
                }
            },
        }
        res = es.search(index=INDEXES, body=body)
        for b in res["aggregations"]["by_pair"]["buckets"]:
            recs.append({
                "g": str(b["key"]["g"]),
                "category": int(b["key"]["category"]) if b["key"].get("category") is not None else None,
                "latest_epoch": b.get("max_collected", {}).get("value"),
            })
        after = res["aggregations"]["by_pair"].get("after_key")
        if not after: break
    return pd.DataFrame.from_records(recs)

def cat_short_map():
    return catmap.set_index("category")["short_name"].to_dict()

def build_counts_table(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["short_name"] = df["category"].map(cat_short_map()).fillna(df["category"].astype(str))
    value_col = "file_docs" if ("ãƒ•ã‚¡ã‚¤ãƒ«æ•°" in count_mode) else "page_docs"
    if display_unit == "å¸‚åŒºç”ºæ‘":
        merged = df.merge(jichitai.rename(columns={"code": "g"}), on="g", how="left")
        pvt = merged.pivot_table(index=["pref_name","city_name","city_type","g"],
                                 columns="short_name", values=value_col, aggfunc="sum",
                                 fill_value=0, observed=True
                                 ).reset_index().sort_values(by=["g"]).drop(columns=["g"])
        pvt["åˆè¨ˆ"] = pvt.drop(columns=["pref_name","city_name","city_type"]).sum(axis=1)
        pvt = pvt.rename(columns={"pref_name":"éƒ½é“åºœçœŒ","city_name":"å¸‚åŒºç”ºæ‘","city_type":"è‡ªæ²»ä½“åŒºåˆ†"})
        ordered = [s for s in short_unique["short_name"].tolist() if s in pvt.columns]
        return pvt[["éƒ½é“åºœçœŒ","å¸‚åŒºç”ºæ‘","è‡ªæ²»ä½“åŒºåˆ†"] + ordered + ["åˆè¨ˆ"]]
    else:
        df["g"] = df["g"].astype(str).str.zfill(2)
        merged = df.merge(pref_master.rename(columns={"affiliation_code":"g"}), on="g", how="left")
        pref_agg = merged.groupby(["g","aff_num","pref_name","short_name"], observed=True)[value_col].sum().reset_index()
        pvt = pref_agg.pivot_table(index=["g","aff_num","pref_name"],
                                   columns="short_name", values=value_col, aggfunc="sum",
                                   fill_value=0, observed=True).reset_index()
        pvt = pvt.sort_values(by=["aff_num","g"])
        pvt["åˆè¨ˆ"] = pvt.drop(columns=["g","aff_num","pref_name"]).sum(axis=1)
        ordered = [s for s in short_unique["short_name"].tolist() if s in pvt.columns]
        pvt = pvt[["pref_name"] + ordered + ["åˆè¨ˆ"]].rename(columns={"pref_name":"éƒ½é“åºœçœŒ"})
        return pvt

def build_latest_table(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["short_name"] = df["category"].map(cat_short_map()).fillna(df["category"].astype(str))
    # epoch â†’ 'YYYYå¹´Mæœˆ'
    df["latest"] = df["latest_epoch"].apply(lambda v: fmt_month_from_epoch(v))
    if display_unit == "å¸‚åŒºç”ºæ‘":
        merged = df.merge(jichitai.rename(columns={"code":"g"}), on="g", how="left")
        pvt = merged.pivot_table(index=["pref_name","city_name","city_type","g"],
                                 columns="short_name", values="latest", aggfunc="max",
                                 fill_value="â€•", observed=True
                                 ).reset_index().sort_values(by=["g"]).drop(columns=["g"])
        pvt = pvt.rename(columns={"pref_name":"éƒ½é“åºœçœŒ","city_name":"å¸‚åŒºç”ºæ‘","city_type":"è‡ªæ²»ä½“åŒºåˆ†"})
        ordered = [s for s in short_unique["short_name"].tolist() if s in pvt.columns]
        return pvt[["éƒ½é“åºœçœŒ","å¸‚åŒºç”ºæ‘","è‡ªæ²»ä½“åŒºåˆ†"] + ordered]
    else:
        df["g"] = df["g"].astype(str).str.zfill(2)
        merged = df.merge(pref_master.rename(columns={"affiliation_code":"g"}), on="g", how="left")
        pref_agg = merged.groupby(["g","aff_num","pref_name","short_name"], observed=True)["latest"].max().reset_index()
        pvt = pref_agg.pivot_table(index=["g","aff_num","pref_name"],
                                   columns="short_name", values="latest", aggfunc="max",
                                   fill_value="â€•", observed=True).reset_index()
        pvt = pvt.sort_values(by=["aff_num","g"])
        ordered = [s for s in short_unique["short_name"].tolist() if s in pvt.columns]
        pvt = pvt[["pref_name"] + ordered].rename(columns={"pref_name":"éƒ½é“åºœçœŒ"})
        return pvt

def show_df(df: pd.DataFrame, latest: bool = False):
    disp = df.copy()
    # æ•°å€¤åˆ—ã¯æ–‡å­—åˆ—åŒ–ã—ã¦ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š
    for c in disp.columns:
        if not latest and pd.api.types.is_numeric_dtype(disp[c]):
            disp[c] = disp[c].apply(lambda v: f"{v:,}" if pd.notnull(v) else "")
    st.dataframe(disp, use_container_width=True, hide_index=True)

def fetch_search_results(query: dict) -> pd.DataFrame:
    """Elastic Searchã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦DataFrameå½¢å¼ã§è¿”ã™"""
    body = {
        "size": result_limit,
        "query": query,
    }
    res = es.search(index=INDEXES, body=body)
    hits = res.get("hits", {}).get("hits", [])
    
    # å¿…è¦ãªæƒ…å ±ã‚’æŠ½å‡º
    data = []
    for hit in hits:
        source = hit["_source"]
        
        # jichitai.xlsxã®codeã‚’6æ¡ã«ã‚¼ãƒ­åŸ‹ã‚ã—ã¦ç…§åˆ
        todofuken = jichitai.loc[jichitai["code"].astype(str).str.zfill(6) == str(source.get("code")).zfill(6), "pref_name"].values
        shikuchoson = jichitai.loc[jichitai["code"].astype(str).str.zfill(6) == str(source.get("code")).zfill(6), "city_name"].values
        
        # category.xlsxã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªåã‚’å–å¾—
        category_name = catmap.loc[catmap["category"] == source.get("category"), "short_name"].values
        
        data.append({
            "å›£ä½“ã‚³ãƒ¼ãƒ‰": str(source.get("code")).zfill(6),  
            "éƒ½é“åºœçœŒ": todofuken[0] if len(todofuken) > 0 else "",
            "å¸‚åŒºç”ºæ‘": shikuchoson[0] if len(shikuchoson) > 0 else "",
            "è³‡æ–™ã‚«ãƒ†ã‚´ãƒª": category_name[0] if len(category_name) > 0 else "",
            "è³‡æ–™å": source.get("title", ""),
            "URL": source.get("source_url", "") + "#page=" + str(source.get("file_page", "")),
            "ãƒšãƒ¼ã‚¸": str(source.get("file_page", "")) + "ï¼" + str(source.get("number_of_pages", "")),
            "æœ¬æ–‡": source.get("content_text", ""),
            "é–‹å§‹å¹´åº¦": source.get("fiscal_year_start", ""),
            "çµ‚äº†å¹´åº¦": source.get("fiscal_year_end", ""),
        })
    
    return pd.DataFrame(data)


# ====== Tabsï¼šä»¶æ•° / æ¤œç´¢çµæœ / æœ€æ–°åé›†æœˆ / AIè¦ç´„ ======
tab_counts, tab_results, tab_latest, tab_summary = st.tabs(["ä»¶æ•°", "æ¤œç´¢çµæœ", "æœ€æ–°åé›†æœˆ", "ğŸ¤– AIè¦ç´„"])

with tab_counts:
    group_field = FIELD_CODE if display_unit == "å¸‚åŒºç”ºæ‘" else FIELD_AFFILIATION
    df_counts = fetch_counts(_qkey(query), group_field, include_file=("ãƒ•ã‚¡ã‚¤ãƒ«æ•°" in count_mode))
    if df_counts.empty:
        st.warning("è©²å½“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ«ã‚¿ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
    else:
        table = build_counts_table(df_counts)
        show_df(table)

with tab_results:
    if query:
        df_results = fetch_search_results(query)
        if df_results.empty:
            st.warning("è©²å½“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ«ã‚¿ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
        else:
            st.dataframe(df_results, use_container_width=True, hide_index=True)
    else:
        st.warning("æ¤œç´¢æ¡ä»¶ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

with tab_latest:
    group_field = FIELD_CODE if display_unit == "å¸‚åŒºç”ºæ‘" else FIELD_AFFILIATION
    df_latest = fetch_latest_month(_qkey(query), group_field)
    if df_latest.empty:
        st.warning("è©²å½“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ«ã‚¿ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
    else:
        table = build_latest_table(df_latest)
        show_df(table, latest=True)

with tab_summary:
    st.subheader("ğŸ¤– Gemini AIã«ã‚ˆã‚‹è¦ç´„")
    
    # APIã‚­ãƒ¼ã®ç¢ºèª
    if not GEMINI_API_KEY:
        st.error("Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit Secretsã« `GEMINI_API_KEY` ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
    else:
        # æ¤œç´¢çµæœã®ç¢ºèª
        if query:
            df_results = fetch_search_results(query)
            
            if df_results.empty:
                st.warning("è¦ç´„ã™ã‚‹æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚æ¤œç´¢æ¡ä»¶ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
            else:
                st.info(f"ğŸ“Š æ¤œç´¢çµæœ: {len(df_results)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
                
                # è¦ç´„ãƒ¢ãƒ¼ãƒ‰é¸æŠ
                summary_mode = st.radio(
                    "è¦ç´„ãƒ¢ãƒ¼ãƒ‰",
                    ["è‡ªå‹•è¦ç´„", "ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"],
                    horizontal=True
                )
                
                # ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å…¥åŠ›
                custom_instruction = ""
                if summary_mode == "ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ":
                    custom_instruction = st.text_area(
                        "AIã¸ã®æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
                        placeholder="ä¾‹: ã“ã‚Œã‚‰ã®æ–‡æ›¸ã‹ã‚‰ç’°å¢ƒæ”¿ç­–ã«é–¢ã™ã‚‹å…±é€šã®èª²é¡Œã‚’3ã¤æŠ½å‡ºã—ã¦ãã ã•ã„",
                        height=100
                    )
                
                # è¦ç´„å®Ÿè¡Œãƒœã‚¿ãƒ³
                if st.button("ğŸš€ è¦ç´„ã‚’å®Ÿè¡Œ", type="primary"):
                    with st.spinner("AIãŒè¦ç´„ã‚’ç”Ÿæˆä¸­..."):
                        try:
                            # Geminiãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
                            model = get_gemini_model(GEMINI_API_KEY)
                            
                            # DataFrameã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                            documents = df_results.to_dict('records')
                            
                            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
                            if summary_mode == "è‡ªå‹•è¦ç´„":
                                prompt = get_summary_prompt(documents)
                            else:
                                if not custom_instruction:
                                    st.error("ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                                    st.stop()
                                prompt = get_custom_prompt(documents, custom_instruction)
                            
                            # è¦ç´„ç”Ÿæˆ
                            summary = generate_summary(model, prompt)
                            
                            if summary:
                                st.success("âœ… è¦ç´„ãŒå®Œæˆã—ã¾ã—ãŸ")
                                
                                # è¦ç´„çµæœã®è¡¨ç¤º
                                st.markdown("### ğŸ“ è¦ç´„çµæœ")
                                st.markdown(summary)
                                
                                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                                st.download_button(
                                    label="ğŸ“¥ è¦ç´„ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                    data=summary,
                                    file_name=f"summary_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                                    mime="text/plain"
                                )
                            else:
                                st.error("è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                                
                        except Exception as e:
                            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                
                # ä½¿ç”¨ä¸Šã®æ³¨æ„
                with st.expander("â„¹ï¸ AIè¦ç´„ã®ä½¿ç”¨ä¸Šã®æ³¨æ„"):
                    st.markdown("""
                    - AIã«ã‚ˆã‚‹è¦ç´„ã¯å‚è€ƒæƒ…å ±ã§ã™ã€‚é‡è¦ãªæ±ºå®šã«ã¯å¿…ãšåŸæ–‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„
                    - æ¤œç´¢çµæœãŒå¤šã„å ´åˆã€å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™
                    - æœ¬æ–‡ã¯æœ€å¤§2000æ–‡å­—ã¾ã§ä½¿ç”¨ã•ã‚Œã¾ã™
                    - Gemini APIã®åˆ©ç”¨åˆ¶é™ã«å¿œã˜ã¦ã€ä¸€åº¦ã«å‡¦ç†ã§ãã‚‹ä»¶æ•°ã«åˆ¶é™ãŒã‚ã‚Šã¾ã™
                    """)
        else:
            st.warning("ã¾ãšæ¤œç´¢æ¡ä»¶ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")